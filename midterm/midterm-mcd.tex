\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{listings}
\lstset{
	numbers=left,
	numbersep=5pt,
	stepnumber=1,
	tabsize=2,
	showstringspaces=false
}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{STA561/CS571}
\newcommand\semester{Fall 2013}     % <-- current semester
% \newcommand\hwnum{3}                  % <-- homework number
\newcommand\yourname{Matt Dickenson} % <-- your name
\newcommand\login{mcd31}           % <-- your NetID
\newcommand\hwdate{Due: 16 October, 2013}           % <-- HW due date

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}


\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ \texttt{\login}\\\course\ --- \semester}
% \chead{\textbf{\Large Homework \hwnum}}
\chead{\textbf{Midterm Exam}}
\rhead{\hwdate}
\headsep 10pt

\begin{document}

\noindent \emph{Notes:} I did not work with anyone else on this exam or refer to resources other than the supplied article, course notes, textbook, and course Piazza page.


\begin{answer}{1}

\paragraph{A} We can write out the expected log likelihood as:
\begin{eqnarray*}
\mathbb{E}[\ell_c] &=& \mathbb{E} [ \log \prod_{i=1}^n (2 \pi)^{p/2} |\Psi|^{-1/2} \exp \{ -\frac{1}{2} [ x_i - \mu - \Lambda z_i]^{\prime} \Psi^{-1} [x_i - \mu - \Lambda z_i] \}] \\
&=& c - \frac{n}{2} \log |\Psi| - \\
&&~~~~~~~~~ \sum_{i=1}^n \mathbb{E} [ \frac{1}{2}(x_i \Psi^{-1} x_i^{\prime} - 2x_i \Psi^{-1}\mu^{\prime} -2x_i\Psi^{-1}\Lambda z_i + \mu \Psi^{-1}\mu^{\prime} + 2\mu \Psi^{-1} \Lambda z_i + z_i^{\prime} \Lambda^{\prime} \Psi^{-1} \Lambda z_i)] \\
&=& c - \frac{n}{2} \log |\Psi| - \\
&&~~~~~~~~~ \sum_{i=1}^n \mathbb{E} [ \frac{1}{2}x_i \Psi^{-1} x_i^{\prime} - x_i \Psi^{-1}\mu^{\prime} -x_i\Psi^{-1}\Lambda z_i + \frac{1}{2} \mu \Psi^{-1}\mu^{\prime} + \mu \Psi^{-1} \Lambda z_i + \frac{1}{2}z_i^{\prime} \Lambda^{\prime} \Psi^{-1} \Lambda z_i] \\
&=& c - \frac{n}{2} \log |\Psi| - \sum_{i=1}^n  \{ \frac{1}{2}x_i \Psi^{-1} x_i^{\prime} - x_i \Psi^{-1}\mu^{\prime} -x_i\Psi^{-1}\Lambda \mathbb{E}[z_i|x_i] + \\ 
&&~~~~~~~~~ \frac{1}{2} \mu \Psi^{-1}\mu^{\prime} + \mu \Psi^{-1} \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2} tr(  \Lambda^{\prime} \Psi^{-1} \Lambda E [z_i z_i^{\prime} | x_i] )\}\\
\end{eqnarray*}

From this we can determine that the expected sufficient statistics are $\mathbb{E}[z_i|x_i]$ and $E [z_i z_i^{\prime} | x_i]$. 

\paragraph{B} We can derive the maximum likelhihood estimated of $\mu$, $\Psi$, and $\Lambda$ by differentiating the expected log likelihood:
\begin{eqnarray*}
{\partial \mathbb{E}[\ell_c] \over \partial \mu } = 0 &=& - \sum_{i=1}^n \{ -x_i \Psi^{-1} + \frac{1}{2} \mu^{(new)} \Psi^{-1} + \Psi^{-1} \} \\
&=& \sum_{i=1}^n x_i \Psi^{-1} - \frac{n}{2} \sum_{i=1}^n 2\mu^{(new)} \Psi^{-1} - \sum_{i=1}^n \Psi^{-1} \mathbb{E}[z_i | x_i] \\
\mu^{(new)} \sum_{i=1}^n \Psi^{-1} &=& \frac{1}{n} \sum_{i=1}^n x_i \Psi^{-1} - \frac{1}{n} \sum_{i=1}^n \Psi^{-1} \mathbb{E}[z_i | x_i] \\
\mu^{(new)} &=& \frac{1}{n} \sum_{i=1}^n x_i - \frac{1}{n} \sum_{i=1}^n \mathbb{E}[z_i | x_i]
\end{eqnarray*}

\begin{eqnarray*}
{\partial \mathbb{E}[\ell_c] \over \partial \Psi^{-1}  } = 0 &=& -\frac{n}{2} \Psi^{(new)}
- \sum_{i=1}^n \{ \frac{1}{2} x_i x_i^{\prime} - x_i\mu^{\prime} - x_i \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2}\mu \mu^{\prime} + \mu \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2} \Lambda^{\prime} \Lambda E [z_i z_i^{\prime} | x_i]  \} \\
\frac{n}{2} \Psi^{(new)} &=&  \sum_{i=1}^n \{ \frac{1}{2} x_i x_i^{\prime} - x_i\mu^{\prime} - x_i \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2}\mu \mu^{\prime} + \mu \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2} \Lambda^{\prime} \Lambda E [z_i z_i^{\prime} | x_i]  \} \\
\Psi^{(new)}  &=& \frac{1}{n} \sum_{i=1}^n \{ x_i(x_i^{\prime} - 2\mu^{\prime} - \Lambda \mathbb{E}[z_i | x_i]) \} + \frac{1}{n} \sum_{i=1}^n \{ \mu ( \mu^{\prime} + 2\Lambda \mathbb{E}[z_i | x_i] )  \} + \frac{1}{n} \sum_{i=1}^n \Lambda^{\prime} \Lambda E [z_i z_i^{\prime} | x_i] \\
\end{eqnarray*}

\begin{eqnarray*}
{\partial \mathbb{E}[\ell_c] \over \partial \Lambda } = 0 &=& - \sum_{i=1}^n \{ -x_i \Psi^{-1} \mathbb{E}[z_i | x_i] + \mu \Psi^{-1} \mathbb{E}[z_i | x_i] + \Lambda^{\prime} \Psi^{-1} E [z_i z_i^{\prime} | x_i] \} \\
n \Lambda^{(new)} \Psi^{-1} E [z_i z_i^{\prime} | x_i] &=& \sum_{i=1}^n \{ -x_i \Psi^{-1} \mathbb{E}[z_i | x_i] + \mu \Psi^{-1} \mathbb{E}[z_i | x_i] \} \\
\Lambda^{(new)}  &=& E [z_i z_i^{\prime} | x_i]^{-1} \frac{1}{n} \sum_{i=1}^n \{ \mathbb{E}[z_i | x_i] (\mu - x_i)  \}  
\end{eqnarray*}

\paragraph{C} Now we derive the expected sufficient statistics, using fact that data and factors are jointly normal:
\begin{eqnarray*}
\mathbb{E}[z|x-\mu] &=& \Lambda^{\prime} (\Psi + \Lambda \Lambda^{\prime})^{-1} x^{\prime} \\
\mathbb{E}[z|x] &=&  \mu + \Lambda^{\prime} (\Psi + \Lambda \Lambda^{\prime})^{-1}  x^{\prime} \\
\mathbb{E}[z z^{\prime}|x] &=& Var(z|x) + \mathbb{E}[z|x]E[z|x]^\prime \\
&=& I + [x(\mu + \Lambda^{\prime}(\Psi + \Lambda \Lambda^{\prime})^{-1})] [(\mu + \Lambda^{\prime}(\Psi + \Lambda \Lambda^{\prime})^{-1})x^{\prime}] 
\end{eqnarray*}


\paragraph{D} Now the EM pseudocode, written with $\mathcal{R}$ syntax for matrix operations and indexing:
\begin{lstlisting}
initialize mu[1:p] = sample(x)
initialize psi = var(x - mu)
initialize lambda = matrix(epsilon)
repeat
  for each example i=1:N do 
    expected_z[i] = mu + t(lambda) %*% 
      (psi + lambda %*% t(lambda)) %*% t(x)
    expected_z_squared[i] = I + (x %*% t(lambda) %*% 
    	(psi + lambda %*% t(lambda))) %*% 
      (t(lambda) %*% (psi + lambda %*% t(lambda)) %*% t(x))
  end 
  for each factor k = 1:K do 
	  for each feature j=1:p do 
	    mu[k][j] = mean(x[,j]) + mean(expected_z[,j])
	    psi[k][j, j] = mean(x%*%(t(x)-2*t(mu)-lambda*mean(expected_z))) + 
	      mean(mu %*% (t(mu) + 2 lambda*mean(expected_z))) + 
	      mean(t(lambda) %*% lambda * mean(expected_z_squared))
	    lambda = factor_loadings(x, z)
	   end 
	end 
until converged 
\end{lstlisting}

I would set $K$ by cross-validation, holding out subsets of $X$ each time. 

\paragraph{E} I would assess convergence by the change in each $\mu_{1:p}$ between iterations. If these are not changing, it suggests that the underlying factors are not changing. 

\end{answer}

\begin{answer}{2}
Modeling mean values of each feature in the model is better than mean-centering each of the features before performing factor analysis because it takes into account $E[z_i|x_i]$, leveraging the information in the factor analysis to compute feature means. This is useful because the some features will be more strongly correlated with some factors than others. 
\end{answer}

\begin{answer}{3}
One problem where a mixture of factor analyzers would be preferable to a simpler factor analysis model is in analyzing voting behavior. For example, some observers of Turkish politics have argued that Turkey is turning its back on the West (specifically the US and EU) in favor of the East, as represented by Russia and Iran. This claim often references voters' preferences for the ruling AKP party, which has reduced the country's efforts to join the EU in favor of a ``zero-problems'' policy with its immediate neighbors including Iran. 

It would be difficult to draw inferences about this debate directly from the individual features that may influence a voter's choice of political party (e.g. gender, education level, income), so factor analysis is preferable to identify certain latent dimensions (e.g. religiosity, favorability toward the West). However, individuals likely differ in the extent to which each of these latent factors motivates them. Thus, a mixture of factor analyzers would be useful. 

The parameters of this model are $\Psi$ and $\Lambda$ as before, $\mu_j$ as the mean of the $j^{th}$ factor analyzer (of a total of $J$), and the vector $\pi$ of mixing proportions. $\Psi$ and $\Lambda$ help to assess the covariance structure but are not of primary interest for the applied problem described above. $\mu$ helps to give a baseline for each factor analyzer. The main parameter of interest is $\pi$, which indicates the weight given to each factor analyzer in the model. 

If much weight is given to a model in which factors appear to be correlated with an East-West divide (i.e. a large value of $\pi_j$), that would lend credibility to one side of the Turkish debate. On the other hand, low weight on such factor analyzers would indicate that the East-West dimension is not a primary cleavage in contemporary Turkish politics. In this way, a mixture of factor analyzers could help shed light on an important political question. 

\end{answer}

\end{document}
