\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{listings}
\lstset{
	numbers=left,
	numbersep=5pt,
	stepnumber=1,
	tabsize=2,
	showstringspaces=false
}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{STA561/CS571}
\newcommand\semester{Fall 2013}     % <-- current semester
% \newcommand\hwnum{3}                  % <-- homework number
\newcommand\yourname{Matt Dickenson} % <-- your name
\newcommand\login{mcd31}           % <-- your NetID
\newcommand\hwdate{Due: 16 October, 2013}           % <-- HW due date

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}


\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ \texttt{\login}\\\course\ --- \semester}
% \chead{\textbf{\Large Homework \hwnum}}
\chead{\textbf{Midterm Exam}}
\rhead{\hwdate}
\headsep 10pt

\begin{document}

\noindent \emph{Notes:} I did not work with anyone else on this exam or refer to resources other than the supplied article, course notes, textbook, and course Piazza page.


\begin{answer}{1}

\paragraph{A} We can write out the expected log likelihood as:
\begin{eqnarray*}
\mathbb{E}[\ell_c] &=& \mathbb{E} [ \log \prod_{i=1}^n (2 \pi)^{p/2} |\Psi|^{-1/2} \exp \{ -\frac{1}{2} [ x_i - \mu - \Lambda z_i]^{\prime} \Psi^{-1} [x_i - \mu - \Lambda z_i] \}] \\
&=& c - \frac{n}{2} \log |\Psi| - \\
&&~~~~~~~~~ \sum_{i=1}^n \mathbb{E} [ \frac{1}{2}(x_i \Psi^{-1} x_i^{\prime} - 2x_i \Psi^{-1}\mu^{\prime} -2x_i\Psi^{-1}\Lambda z_i + \mu \Psi^{-1}\mu^{\prime} + 2\mu \Psi^{-1} \Lambda z_i + z_i^{\prime} \Lambda^{\prime} \Psi^{-1} \Lambda z_i)] \\
&=& c - \frac{n}{2} \log |\Psi| - \\
&&~~~~~~~~~ \sum_{i=1}^n \mathbb{E} [ \frac{1}{2}x_i \Psi^{-1} x_i^{\prime} - x_i \Psi^{-1}\mu^{\prime} -x_i\Psi^{-1}\Lambda z_i + \frac{1}{2} \mu \Psi^{-1}\mu^{\prime} + \mu \Psi^{-1} \Lambda z_i + \frac{1}{2}z_i^{\prime} \Lambda^{\prime} \Psi^{-1} \Lambda z_i] \\
&=& c - \frac{n}{2} \log |\Psi| - \sum_{i=1}^n  \{ \frac{1}{2}x_i \Psi^{-1} x_i^{\prime} - x_i \Psi^{-1}\mu^{\prime} -x_i\Psi^{-1}\Lambda \mathbb{E}[z_i|x_i] + \\ 
&&~~~~~~~~~ \frac{1}{2} \mu \Psi^{-1}\mu^{\prime} + \mu \Psi^{-1} \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2} tr(  \Lambda^{\prime} \Psi^{-1} \Lambda E [z_i z_i^{\prime} | x_i] )\}\\
\end{eqnarray*}

From this we can determine that the expected sufficient statistics are $\mathbb{E}[z_i|x_i]$ and $E [z_i z_i^{\prime} | x_i]$. 

\paragraph{B} We can derive the maximum likelhihood estimated of $\mu$, $\Psi$, and $\Lambda$ by differentiating the expected log likelihood:
\begin{eqnarray*}
{\partial \mathbb{E}[\ell_c] \over \partial \mu^{(new)} } = 0 &=& - \sum_{i=1}^n \{ -x_i \Psi^{-1} + \frac{1}{2} \mu \Psi^{-1} + \Psi^{-1} \} \\
&=& \sum_{i=1}^n x_i \Psi^{-1} - \frac{n}{2} \sum_{i=1}^n 2\mu^{(new)} \Psi^{-1} - \sum_{i=1}^n \Psi^{-1} \mathbb{E}[z_i | x_i] \\
\mu^{(new)} \sum_{i=1}^n \Psi^{-1} &=& \frac{1}{n} \sum_{i=1}^n x_i \Psi^{-1} - \frac{1}{n} \sum_{i=1}^n \Psi^{-1} \mathbb{E}[z_i | x_i] \\
\mu^{(new)} &=& \frac{1}{n} \sum_{i=1}^n x_i - \frac{1}{n} \sum_{i=1}^n \mathbb{E}[z_i | x_i]
\end{eqnarray*}

\begin{eqnarray*}
{\partial \mathbb{E}[\ell_c] \over \partial \Psi^{-1}  } = 0 &=& -\frac{n}{2} \Psi^{(new)}
- \sum_{i=1}^n \{ \frac{1}{2} x_i x_i^{\prime} - x_i\mu^{\prime} - x_i \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2}\mu \mu^{\prime} + \mu \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2} \Lambda^{\prime} \Lambda E [z_i z_i^{\prime} | x_i]  \} \\
\frac{n}{2} \Psi^{(new)} &=&  \sum_{i=1}^n \{ \frac{1}{2} x_i x_i^{\prime} - x_i\mu^{\prime} - x_i \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2}\mu \mu^{\prime} + \mu \Lambda \mathbb{E}[z_i|x_i] + \frac{1}{2} \Lambda^{\prime} \Lambda E [z_i z_i^{\prime} | x_i]  \} \\
\Psi^{(new)}  &=& \frac{1}{n} \sum_{i=1}^n \{ x_i(x_i^{\prime} - 2\mu^{\prime} - \Lambda \mathbb{E}[z_i | x_i]) \} + \frac{1}{n} \sum_{i=1}^n \{ \mu ( \mu^{\prime} + 2\Lambda \mathbb{E}[z_i | x_i] )  \} + \frac{1}{n} \sum_{i=1}^n \Lambda^{\prime} \Lambda E [z_i z_i^{\prime} | x_i] \\
\end{eqnarray*}

\begin{eqnarray*}
{\partial \mathbb{E}[\ell_c] \over \partial \Lambda } = 0 &=& - \sum_{i=1}^n \{ -x_i \Psi^{-1} \mathbb{E}[z_i | x_i] + \mu \Psi^{-1} \mathbb{E}[z_i | x_i] + \Lambda^{\prime} \Psi^{-1} E [z_i z_i^{\prime} | x_i] \} \\
n \Lambda^{(new)} \Psi^{-1} E [z_i z_i^{\prime} | x_i] &=& \sum_{i=1}^n \{ -x_i \Psi^{-1} \mathbb{E}[z_i | x_i] + \mu \Psi^{-1} \mathbb{E}[z_i | x_i] \} \\
\Lambda^{(new)}  &=& E [z_i z_i^{\prime} | x_i]^{-1} \frac{1}{n} \sum_{i=1}^n \{ \mathbb{E}[z_i | x_i] (\mu - x_i)  \}  
\end{eqnarray*}

\paragraph{C} Now we derive the expected sufficient statistics, using fact that data and factors are jointly normal:
\begin{eqnarray*}
\mathbb{E}[z|x-\mu] &=& \Lambda^{\prime} (\Psi + \Lambda \Lambda^{\prime})^{-1} x^{\prime} \\
\mathbb{E}[z|x] &=&  \mu + \Lambda^{\prime} (\Psi + \Lambda \Lambda^{\prime})^{-1}  x^{\prime} \\
\mathbb{E}[z z^{\prime}|x] &=& Var(z|x) + \mathbb{E}[z|x]E[z|x]^\prime \\
&=& I + [x(\mu + \Lambda^{\prime}(\Psi + \Lambda \Lambda^{\prime})^{-1})] [(\mu + \Lambda^{\prime}(\Psi + \Lambda \Lambda^{\prime})^{-1})x^{\prime}] 
\end{eqnarray*}


\paragraph{D} Now the EM pseudocode, written with $\mathcal{R}$ syntax for matrix operations and indexing:
\begin{lstlisting}
initialize mu[1:p] = sample(x, n/10)
initialize psi = var(x - mu)
initialize lambda = matrix(epsilon)
repeat
  for each example i=1:N do 
    expected_z[i] = mu + t(lambda) %*% 
      (psi + lambda %*% t(lambda)) %*% t(x)
    expected_z_squared[i] = I + (x %*% t(lambda) %*% 
    	(psi + lambda %*% t(lambda))) %*% 
      (t(lambda) %*% (psi + lambda %*% t(lambda)) %*% t(x))
  end 
  for each factor k = 1:K do 
	  for each feature j=1:p do 
	    mu[k][j] = mean(x[,j]) + mean(expected_z[,j])
	    psi[k][j, j] = mean(x%*%(t(x)-2*t(mu)-lambda*mean(expected_z))) + 
	      mean(mu %*% (t(mu) + 2 lambda*mean(expected_z))) + 
	      mean(t(lambda) %*% lambda * mean(expected_z_squared))
	    lambda = factor_loadings(x, z)
	   end 
	end 
until converged 
\end{lstlisting}

I would set $K$ by cross-validation, holding out subsets of $X$ each time. 

\paragraph{E} I would assess convergence by the change in each $\mu_{1:p}$ between iterations. If these are not changing, it suggests that the underlying factors are not changing. 

\end{answer}

\begin{answer}{2}
Modeling mean values of each feature in the model is better than mean-centering each of the features before performing factor analysis because it takes into account $E[z_i|x_i]$. That is, it leverages the information in the factor analysis to compute feature means. 
\end{answer}

\begin{answer}{3}

\end{answer}

\end{document}
