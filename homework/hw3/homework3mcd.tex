\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{listings}
\lstset{
	numbers=left,
	numbersep=5pt,
	stepnumber=1,
	tabsize=2,
	showstringspaces=false
}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{STA561/CS571}
\newcommand\semester{Fall 2013}     % <-- current semester
\newcommand\hwnum{2}                  % <-- homework number
\newcommand\yourname{Matt Dickenson} % <-- your name
\newcommand\login{mcd31}           % <-- your NetID
\newcommand\hwdate{Due: 23 September, 2013}           % <-- HW due date

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}


\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ \texttt{\login}\\\course\ --- \semester}
\chead{\textbf{\Large Homework \hwnum}}
\rhead{\hwdate}
\headsep 10pt

\begin{document}

\noindent \emph{Homework Notes:} I did not work with anyone else on this homework or refer to resources other than the course notes, textbook, and course Piazza page. This homework took me $X+2$ hours, where $X$ is the amount of time it took to complete the original assignment and submit it on Friday, and 2 additional hours revising when homework problems were changed several days after being assigned.

\begin{answer}{1}

\paragraph{A} Using the normal equation $\hat{\beta}=(X^TX)^{-1}(X^TY)$, we can calculate $\hat{\beta}$ in $\mathcal{R}$ with the following program (after loading the data as \texttt{lindata}):

\lstinputlisting[language=R, caption=R Code for 1A, firstline=13, lastline=25, firstnumber=1]{cs571-hw3.r}

Letting $\beta_i$ correspond to $X_i$ and $\beta_0$ represent the intercept, $\hat{\beta}_0=69.94, \hat{\beta}_1=15.12, \hat{\beta}_2=0.32$. 


\paragraph{B}  We can also estimate $\beta$ using online stochastic gradient descent with $\mathcal{R}$'s \texttt{optim} function for minimization:

\lstinputlisting[language=R, caption=R Code for 1B, firstline=28, lastline=42, firstnumber=last]{cs571-hw3.r}

Using this method we reach the same results as above: $\hat{\beta}_0=69.94, \hat{\beta}_1=15.12, \hat{\beta}_2=0.32$. 

\paragraph{C} A third method we can use to estimate $\beta$ is ridge regression, using the following $\mathcal{R}$ code:

\lstinputlisting[language=R, caption=R Code for 1C, firstline=45, lastline=56, firstnumber=last]{cs571-hw3.r}

From this approach we get the estimates $\hat{\beta}_1=3.00, \hat{\beta}_2=0.78$. 

\paragraph{D} Table \ref{rss.train} presents the residual sum of squares (RSS) for the training data using each of the methods above.


\begin{table}[h!]
\caption{Training set RSS for linear regression methods}
\label{rss.train}
\begin{center}
\begin{tabular}{lr}
Method & RSS \\
\hline
Normal equations & 98,729.3 \\
Online stochastic gradient descent & 98,729.3 \\
Ridge regression & 99,005.9
\end{tabular}
\end{center}
\end{table}

% RSS(\beta) = \sum_{i=1}^n (y_i - \beta^T x_i)^2

\paragraph{E} Table \ref{rss.test} presents the residual sum of squares (RSS) for the test data using each of the methods above.

\begin{table}[h!]
\caption{Test set RSS for linear regression methods}
\label{rss.test}
\begin{center}
\begin{tabular}{lr}
Method & RSS \\
\hline
Normal equations & 11,573.2 \\
Online stochastic gradient descent & 11,573.2 \\
Ridge regression & 11,523.5
\end{tabular}
\end{center}
\end{table}

\paragraph{F}

We can now take the predicted blood pressure for a hypothetical female weighing 135 pounds for the three different methods (Table \ref{pred}):

\begin{table}[h!]
\caption{Predicted values}
\label{pred}
\begin{center}
\begin{tabular}{lr}
Method & $\mathbb{E}[Y|X=[1,135]^T,\hat{\beta}]$ \\
\hline
Normal equations & 128.4 \\
Online stochastic gradient descent & 128.4 \\
Ridge regression & 127.2
\end{tabular}
\end{center}
\end{table}

\paragraph{G} Bivariate plots are given in Figures \ref{p1x1} and \ref{p1x2} below, along with regression lines for the three methods used above. The $y$-axis values are $Y-\hat{\beta}_i X_i - \hat{\beta}_0$ and the $x$-axis values are $X_j, j \neq i$. (Note that indices in the $y$-axis label do not correspond to the $X$ and $\hat{\beta}$ subscripts because $\mathcal{R}$ uses 1-based indexing.) Regression lines in the plots for $X_i$ use $\hat{\beta}_i X_i$

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.4]{p1x1-normal.pdf}
\includegraphics[scale=0.4]{p1x1-sgd.pdf}
\includegraphics[scale=0.4]{p1x1-ridge.pdf}
\caption{$X_1$ and $Y$ with Regression Lines}
\label{p1x1}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.4]{p1x2-normal.pdf}
\includegraphics[scale=0.4]{p1x2-sgd.pdf}
\includegraphics[scale=0.4]{p1x2-ridge.pdf}\caption{$X_2$ and $Y$ with Regression Lines}
\label{p1x2}
\end{center}
\end{figure}

\paragraph{H} Of the three methods used above, ridge regression is the least likely to overfit the data. This is because the $\lambda$ term (using the notation in the MLAPP textbook) helps to regularize the coefficients. This leads to higher RSS in the training data but also makes the coefficients less susceptible to outliers, as evidenced by the lower RSS using the test set data. The other two methods provide no such protection against outliers, and are thus more likely to overfit the training data. 

\paragraph{I} The researchers should use the ridge regresion results. One reason for this is its robustness (relative to the other two methods) to overfitting. This is especially pertinent given the small $n$ of the training data. However, the similarity in the coefficients should help allay any concerns the researchers may have over the differences in the three methods. 

\end{answer}



\newpage
\begin{answer}{2} 

\paragraph{A} The following $\mathcal{R}$ code implements the IRLS algorithm as presented in the MLAPP textbook (after loading the data):

\lstinputlisting[language=R, caption=R Code for 2A, firstline=168, lastline=222, firstnumber=1]{cs571-hw3.r}

\paragraph{B} The RSS($\hat{\beta}$) for the training data is 141.47. 

\paragraph{C} The RSS($\hat{\beta}$) for the test data is 15.73.

\paragraph{D} $\mathbb{E}[Z|X=[1,135]^T,\hat{\beta}] = 0.088$

\paragraph{E} Bivariate plots are given in Figures \ref{p2x1} and \ref{p2x2} below, along with regression lines from the IRLS results. The $y$-axis values are $Z-\hat{\beta}_i X_i - \hat{\beta}_0$ and the $x$-axis values are $X_j, j \neq i$. (Note that indices in the $y$-axis label do not correspond to the $X$ and $\hat{\beta}$ subscripts because $\mathcal{R}$ uses 1-based indexing.) Regression lines in the plots for $X_i$ use $\hat{\beta}_i X_i$

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.5]{p2x1.pdf}
\caption{$X_1$ and $Z$ with Regression Line}
\label{p2x1}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.5]{p2x2.pdf}
\caption{$X_2$ and $Z$ with Regression Line}
\label{p2x2}
\end{center}
\end{figure}

\paragraph{F} The correlation between the features $X_1$ and $X_2$ is high (0.63). This causes our estimates of $\beta$ to be biased, since it is difficult to isolate the impact of each feature. If the features are gender and weight, this is because the mean of the male weight distribution is higher than that for females. However, both features are still important to the analysis. 

\paragraph{G} Although $X_1$ and $X_2$ are correlated, both features are important for the decision rule used to predict $Z^*$. Assuming $X_1$ is gender (female=1) and $X_2$ is weight, the decision rule I would recommend (based on Figure \ref{p2x2}) is that females with a weight over about 145 pounds and males with a weight over about 160 pounds have a high risk of $Z$. These patients should be counseled to seek additional diagnostic tests. 
 

\end{answer}




\end{document}
