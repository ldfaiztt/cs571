\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{listings}
\lstset{
	numbers=left,
	numbersep=5pt,
	stepnumber=1,
	tabsize=2,
	showstringspaces=false
}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{STA561/CS571}
\newcommand\semester{Fall 2013}     % <-- current semester
\newcommand\hwnum{2}                  % <-- homework number
\newcommand\yourname{Matt Dickenson} % <-- your name
\newcommand\login{mcd31}           % <-- your NetID
\newcommand\hwdate{Due: 23 September, 2013}           % <-- HW due date

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}


\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ \texttt{\login}\\\course\ --- \semester}
\chead{\textbf{\Large Homework \hwnum}}
\rhead{\hwdate}
\headsep 10pt

\begin{document}

\noindent \emph{Homework Notes:} I did not work with anyone else on this homework or refer to resources other than the course notes, textbook, and course Piazza page.

\begin{answer}{1}

\paragraph{A} Using the normal equation $\hat{\beta}=(X^TX)^{-1}(X^TY)$, we can calculate $\hat{\beta}$ in $\mathcal{R}$ with the following program (after loading the data as \texttt{lindata}):

\lstinputlisting[language=R, caption=R Code for 1A, firstline=16, lastline=23, firstnumber=1]{cs571-hw3.r}

Letting $\beta_i$ correspond to $X_i$ and $\beta_0$ represent the intercept, $\hat{\beta}_0=69.94, \hat{\beta}_1=15.12, \hat{\beta}_2=0.32$. 


\paragraph{B}  We can also estimate $\beta$ using online stochastic gradient descent using $\mathcal{R}$'s \texttt{optim} function for minimization:

\lstinputlisting[language=R, caption=R Code for 1B, firstline=26, lastline=40, firstnumber=last]{cs571-hw3.r}

Using this method we reach the same results as above: $\hat{\beta}_0=69.94, \hat{\beta}_1=15.12, \hat{\beta}_2=0.32$. 

\paragraph{C} A third method we can use to estimate $\beta$ is ridge regression, using the following $\mathcal{R}$ code:

\lstinputlisting[language=R, caption=R Code for 1C, firstline=43, lastline=54, firstnumber=last]{cs571-hw3.r}

From this approach we get the estimates $\hat{\beta}_1=3.00, \hat{\beta}_2=0.78$. 

\paragraph{D} Table \ref{rss.train} presents the residual sum of squares (RSS) for the training data using each of the methods above.


\begin{table}[h!]
\caption{Training set RSS for linear regression methods}
\label{rss.train}
\begin{center}
\begin{tabular}{lr}
Method & RSS \\
\hline
Normal equations & 98,729.3 \\
Online stochastic gradient descent & 98,729.3 \\
Ridge regression & 99,005.9
\end{tabular}
\end{center}
\end{table}

% RSS(\beta) = \sum_{i=1}^n (y_i - \beta^T x_i)^2

\paragraph{E} Table \ref{rss.test} presents the residual sum of squares (RSS) for the test data using each of the methods above.

\begin{table}[h!]
\caption{Test set RSS for linear regression methods}
\label{rss.test}
\begin{center}
\begin{tabular}{lr}
Method & RSS \\
\hline
Normal equations & 11,573.2 \\
Online stochastic gradient descent & 11,573.2 \\
Ridge regression & 11,523.5
\end{tabular}
\end{center}
\end{table}

\paragraph{F}

We can now take the predicted blood pressure for a hypothetical female weighing 135 pounds:

\begin{table}[h!]
\caption{Predicted values}
\label{pred}
\begin{center}
\begin{tabular}{ll}
Method & $\mathbb{E}[Y|X=[1,135]^T,\hat{\beta}]$ \\
\hline
Normal equations & 128.4 \\
Online stochastic gradient descent & 128.4 \\
Ridge regression & 127.2
\end{tabular}
\end{center}
\end{table}

\paragraph{G} \textbf{PLOTS HERE}

\paragraph{H} Of the three methods used above, ridge regression is the least likely to overfit the data. This is because the $\lambda$ term (using the notation in the MLAPP textbook) helps to regularize the coefficients. This leads to higher RSS but also makes the coefficients less susceptible to outliers in the training data, as evidenced by the lower RSS using the training set data. The other two methods provide no such protection against outliers, and are thus more likely to overfit the training data. 

\paragraph{I} The researchers should use the ridge regresion results. One reason for this is its robustness (relative to the other two methods) to overfitting. This is especially pertinent given the small $n$ of the training data. However, the similarity in the coefficients should help allay any concerns the researchers may have over the differences in the three methods. 

\end{answer}



\pagebreak
\begin{answer}{2} 

\paragraph{A} The following $\mathcal{R}$ code implements the IRLS algorithm as presented in the MLAPP textbook (after loading the data):

\lstinputlisting[language=R, caption=R Code for 2A, firstline=116, lastline=166, firstnumber=1]{cs571-hw3.r}

\paragraph{B} The RSS($\hat{\beta}$) for the training data is 141.47. 

\paragraph{C} The RSS($\hat{\beta}$) for the test data is 15.73.

\paragraph{F} (\emph{sic}) $\mathbb{E}[Z|X=[1,135]^T,\hat{\beta}] = 0.088$

\paragraph{G} \textbf{PLOTS HERE}

\paragraph{H} The correlation between the features $X_1$ and $X_2$ is high (0.63). This causes our estimates of $\beta$ to be biased, since it is difficult to isolate the impact of each feature. 

\end{answer}




\end{document}
