\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{listings}
\lstset{
	numbers=left,
	numbersep=5pt,
	stepnumber=1,
	tabsize=2
}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{STA561/CS571}
\newcommand\semester{Fall 2013}     % <-- current semester
\newcommand\hwnum{2}                  % <-- homework number
\newcommand\yourname{Matt Dickenson} % <-- your name
\newcommand\login{mcd31}           % <-- your NetID
\newcommand\hwdate{Due: 23 September, 2013}           % <-- HW due date

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}


\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ \texttt{\login}\\\course\ --- \semester}
\chead{\textbf{\Large Homework \hwnum}}
\rhead{\hwdate}
\headsep 10pt

\begin{document}

\noindent \emph{Homework Notes:} I did not work with anyone else on this homework or refer to resources other than the course notes, textbook, and course Piazza page.

\begin{answer}{1}

\paragraph{A} Using the normal equation $\hat{\beta}=(X^TX)^{-1}(X^TY)$, we can calculate $\hat{\beta}$ in $\mathcal{R}$ with the following program (after loading the data as \texttt{lindata}):

\lstinputlisting[language=R, caption=R Code for 1A, firstline=16, lastline=22, firstnumber=1]{cs571-hw3.r}

Letting $\beta_i$ correspond to $X_i$, $\hat{\beta}_1=3.01, \hat{\beta}_2=0.78$. 


\paragraph{B}  We can also estimate $\beta$ using online stochastic gradient descent using $\mathcal{R}$'s \texttt{optim} function for minimization:

\lstinputlisting[language=R, caption=R Code for 1B, firstline=25, lastline=39, firstnumber=last]{cs571-hw3.r}

Using this method we reach the same results as above: $\hat{\beta}_1=3.01, \hat{\beta}_2=0.78$. 

\paragraph{C} A third method we can use to estimate $\beta$ is ridge regression, using the following $\mathcal{R}$ code:

\lstinputlisting[language=R, caption=R Code for 1C, firstline=42, lastline=54, firstnumber=last]{cs571-hw3.r}

From this approach we get the estimates $\hat{\beta}_1=3.00, \hat{\beta}_2=0.78$. 

\paragraph{D} Table \ref{rss.train} presents the residual sum of squares (RSS) for the training data using each of the methods above.


\begin{table}[h!]
\caption{Training set RSS for linear regression methods}
\label{rss.train}
\begin{center}
\begin{tabular}{ll}
Method & RSS \\
\hline
Normal equations & 181767.9 \\
Online stochastic gradient descent & 181767.9 \\
Ridge regression & 181768
\end{tabular}
\end{center}
\end{table}

% RSS(\beta) = \sum_{i=1}^n (y_i - \beta^T x_i)^2

\paragraph{E} Table \ref{rss.test} presents the residual sum of squares (RSS) for the training data using each of the methods above.

\begin{table}[h!]
\caption{Test set RSS for linear regression methods}
\label{rss.test}
\begin{center}
\begin{tabular}{ll}
Method & RSS \\
\hline
Normal equations & 16963.35 \\
Online stochastic gradient descent & 16963.35 \\
Ridge regression & 16963.59
\end{tabular}
\end{center}
\end{table}

\paragraph{F}

We can now take the predicted blood pressure for a hypothetical female weighing 135 pounds:

\begin{table}[h!]
\caption{Predicted values}
\label{pred}
\begin{center}
\begin{tabular}{ll}
Method & $\mathbb{E}[Y|X=[1,135]^T,\hat{\beta}]$ \\
\hline
Normal equations & 108.76 \\
Online stochastic gradient descent & 108.76 \\
Ridge regression & 108.75
\end{tabular}
\end{center}
\end{table}

\paragraph{G} \textbf{PLOTS HERE}

\paragraph{H} Of the three methods used above, ridge regression is the least likely to overfit the data. This is because the $\lambda$ term (using the notation in the MLAPP textbook) helps to regularize the coefficients. This leads to higher RSS but also makes the coefficients less susceptible to outliers in the training data. The other two methods provide no such protection against outliers. 

\paragraph{I} The researchers should use the ridge regresion results. One reason for this is its robustness (relative to the other two methods) to overfitting. This is especially pertinent given the relatively small $n$ of the training data. However, the similarity in the coefficients should help allay any concerns the researchers may have over the differences in the three methods. 

\end{answer}



\pagebreak
\begin{answer}{2} 
\end{answer}




\end{document}
