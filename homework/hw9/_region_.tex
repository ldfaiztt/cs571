\message{ !name(homework9mcd.tex)}\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{listings}
\lstset{
	numbers=left,
	numbersep=5pt,
	stepnumber=1,
	tabsize=2,
	showstringspaces=false
}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{natbib}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{STA561/CS571}
\newcommand\semester{Fall 2013}     % <-- current semester
\newcommand\hwnum{9}                  % <-- homework number
\newcommand\yourname{Matt Dickenson} % <-- your name
\newcommand\login{mcd31}           % <-- your NetID
\newcommand\hwdate{Due: 25 November, 2013}           % <-- HW due date

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}


\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ \texttt{\login}\\\course\ --- \semester}
\chead{\textbf{\Large Homework \hwnum}}
\rhead{\hwdate}
\headsep 10pt

\begin{document}

\message{ !name(homework9mcd.tex) !offset(-3) }


\noindent \emph{Homework Notes:} I did not work with anyone else on this homework or refer to resources other than the course notes, textbook, and course Piazza page.

\begin{answer}{1}
% Statement of the problem: Given a data set X ∈ R1000×1 with an unknown number of cluster components, propose an algorithm which simultaneously identifies the number of cluster components and clusters the data.

% Now that we know about Dirichlet processes, we will put a DP prior on the partition of our data into clusters. Let us put this model in a probabilistic framework: let each cluster represent a univariate Gaussian distribution; this model will then represent an infinite Gaussian mixture model. In this mapping to our Chinese Restaurant Process metaphor, a table at a restaurant is a cluster, and a data point is a customer.

\paragraph{A}
% Write out the generative model. What is a simple choice of base distribution?
The generative model for a continuous $\eta$, a base distribution $G_0$, concentration parameter $\alpha$, and the $\{B_1,...B_K\}$ partitions ($K=\inf$), is:
\begin{eqnarray*}
(G(\eta \in B_1), ..., G(\eta \in B)K) &\sim& \text{Dirich}(\alpha G_0 (B_1), ... \alpha G_0 (B_K)) \\
p(\eta_i \in B_j) &=& \int p(\eta_i \in B_j | G) p(G|G_0) dG \\
&=& {\alpha G_0 (B_j) \over \sum_{K} \alpha G_0 (B_k)} \\
&\propto& \alpha G_0 (B_j)  
\end{eqnarray*}

The posterior is
\begin{eqnarray*}
G|\eta_{1:n}, \alpha, G_0 &\sim& DP(\alpha, G_0 + \sum_{i=1}^n \delta_{\eta_i} (\eta))
\end{eqnarray*}

A simple choice for the base distribution is $G_0$ is the Gamma distribution, due to the conjugacy of the Gamma distribution with the Gaussian distribution. 

\paragraph{B}
% Write out the equation that you would use in your Gibbs sampler for cluster as- signment. Exploit exchangeability (i.e. treat each data point as if it were the last to arrive at the restaurant, then remove it from it’s current cluster assignment, and reassign it to a cluster using this equation).

For the cluster assignment step in the Gibbs sampler, we can exploit exchangeability. 


\begin{lstlisting}
cluster = function(x, centroids, alpha){
	# do the restaurant process 
	table_counts = restaurant(x, alpha)
	table_props = table_counts/sum(table_counts)
	num_tables = length(table_counts)

	# then exploit permutation
	permuted_x = sample(x)
	n = length(x)
	table_assignments = rep(NA, n)

	# pretend each x is last to arrive 
	for(i in 1:length(permuted_x)){
		table_i = sample(c(1:num_tables), 1, prob=table_props)
		table_assignments[i] = table_i 
	}
}

restaurant = function(x, alpha){
	table_counts = c(1) # number of 'customers' at each 'table'
											# first customer at first table 
	for(m in 2:n){
		tmp = c(table_counts, alpha)
		table_props = tmp/sum(tmp)

		# assign each 'customer' to a 'table' according to crp
		table_m = sample(c(1:length(tmp)), 1, prob=table_props)
		if(table_m==length(tmp)){ table_counts[table_m] = 1}
		else{ table_counts[table_m] = table_counts[table_m] + 1}
	}
	return(table_counts) # sufficient statistic
}
\end{lstlisting}

\paragraph{C}
% How does this algorithm handle empty clusters?

This algorithm does not discard empty clusters. Because the number of clusters is potentially infinite (in theory), it is possible for an observation to be assigned to a previously empty cluster at any iteration of the Gibbs sampler.

\paragraph{D}
% Explain how this model addresses the problems we encountered in Homework 4, including how this approach differs from specifying a penalty to cluster size of the form: ‘remove a cluster if it has fewer than γ points assigned to it’.

Rather than discarding clusters with fewer than $\gamma$ points assigned to them, we allow the number of potential clusters to be infinite. Thus, this model better addresses the issue of not knowing the number of clusters \emph{a priori}. If we examine the number of clusters at each iteration of the Gibbs sampler (or across multiple runs), we can even get a posterior distribution over the number of clusters. 

\end{answer}


\begin{answer}{2}
% You should now have dived into the analysis of your data and chosen a modeling framework and an validation approach. An important step in the research process is to look at what other ideas researchers have used to approach a related problem. In addition to making progress on your project this week, you will perform a literature search to see what other researchers have worked with similar data or related problems. 

% What relevant approaches, feature sets, or kernels have they developed that might be useful in your own analysis? 

As one of the most widely used dependent variables in international conflict studies, much effort has been devoted to estimating models of MID onset and duration. However, this work suffers from several common weaknesses that this project attempts to ameliorate: virtually all projects, especially before the present decade, used a fixed functional form (typically from the family of generalized linear models); out-of-sample testing and cross-validation is used only rarely, making claims of `prediction' somewhat dubious in many cases; and often the independent variables are measured at the annual level with high levels of serial correlation, meaning that there is little temporal variation in the predictors, while the dependent variable tends to exhibit more sudden onsets \citep{ward2010perils}. A recent shift toward event data has helped to address the latter two of these issues: with frequent updates (often measured at the daily level), there is substantial variation in the independent variables, validation requires only a brief waiting period for new sets of test data \citep{gerner1994,gerner:etal:2002,king2003automated,ruggeri2011events,schrodt2013gdelt}. 

% What modeling approaches and simplifying assumptions worked in this related work, and what didn’t work? 
With this transition toward event data as predictors, the political forecasting community has become attune to new challenges and has responded with several established practices. Coding the sentiment of interactions can now be done in near real-time (NRT) using the Tabari system, which aggregates and deduplicates news reports \citep{o2010crisis,schrodt2009tabari}. Sentiment coding can be done according to two widely used systems. The Goldstein scale assigns a score of -10 (highly conflictual) to +10 (highly cooperative) to events, but it is difficult to employ this scale for aggregations or permutations of the data \citep{goldstein1992conflict}. CAMEO classifies events into a pre-defined schema of material/verbal and cooperative/conflictual actions, that makes aggregation simpler because we can count events within each category \citep{gerner:etal:2002}. These event classifications provide a principled, automated method for exhaustively categorizing  the types of events that may consitute an interstate dispute \citep{ghosn2004mid3}.

The community has also dealt with challenges when aggregating event data up to various temporal levels. Although there is no single best practice, monthly aggregation has become a common strategy \citep{arva2013improving,yonamine2013event} and is used in this project. Modfiying the features by transforming the raw counts into month-to-month changes (i.e. first-differencing) and measuring the balance between conflictual and cooperative interactions as a percentage of the total also helped to simplify the feature set \citep{Box:1976}. 

% What can you take from the scientific literature to your project? 

Interpretability is an important concern in this project due to the policy-relevant nature of the problem and the (potential) need to compare the resulting model to the process used by human coders involved in creating the MID dataset \citep{ghosn2004mid3}. For this reason, ``black box'' methods such as Support Vector Machines were judged to be inappropriate. Classification trees (and their continuous counterpart, regression trees, collectively known as CART) offer a nice alternative that is more flexible than GLMs and more interpretable than Random Forests (these two methods should provide lower and upper bounds, respectively, on CART) \citep{klebanov2008lexical}. Other uses of CART for event data within conflict studies include \citet{schrodt1990predicting,trappl1996digging}; similar circumstances in public health--unbalanced and missing data--have led to the use of CART there as well \citep{speybroeck2012classification}.

In later stages of this project, several additional tools may help to improve the predictive accuracy of the model. International conflict is a relatively rare event, meaning that in $k$-fold cross validation it is possible that some subsets will have no instances of conflict; to prevent this, synthetic minority over-sampling (SMOTE) could be used \citep{chawla2002smote}. To incorporate interdependencies not captured at the dyadic level, future iterations could also include lags that measure conflict in social or spatial neighbors \citep{gleditsch2000war,gleditsch2001measuring,hoff2004modeling,ward1998democratizing,ward2007disputes,ward2011network}. A Bayesian ensemble model of several classification trees could also improve performance while still maintaining more interpretability than is available in random forests\citep{arva2013improving,montgomery2012improving,Raftery:1995,raftery2005using}. If these methods are successful, the general processing of automating political indicators through the use of event data could also be applied to other widely used indices such as the Polity and Freedom House regime scales (measuring democracy and autocracy). 



\end{answer}

\subsubsection*{References}

\begingroup
\renewcommand{\section}[2]{}
\bibliographystyle{unsrt}
\bibliography{/Users/mcdickenson/Documents/Templates/RefLib.bib}
\endgroup


\end{document}
\message{ !name(homework9mcd.tex) !offset(-160) }
