\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{listings}
\lstset{
	numbers=left,
	numbersep=5pt,
	stepnumber=1,
	tabsize=2,
	showstringspaces=false
}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{STA561/CS571}
\newcommand\semester{Fall 2013}     % <-- current semester
\newcommand\hwnum{9}                  % <-- homework number
\newcommand\yourname{Matt Dickenson} % <-- your name
\newcommand\login{mcd31}           % <-- your NetID
\newcommand\hwdate{Due: 25 November, 2013}           % <-- HW due date

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}


\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ \texttt{\login}\\\course\ --- \semester}
\chead{\textbf{\Large Homework \hwnum}}
\rhead{\hwdate}
\headsep 10pt

\begin{document}

\noindent \emph{Homework Notes:} I did not work with anyone else on this homework or refer to resources other than the course notes, textbook, and course Piazza page.

\begin{answer}{1}
% Statement of the problem: Given a data set X ∈ R1000×1 with an unknown number of cluster components, propose an algorithm which simultaneously identifies the number of cluster components and clusters the data.

% Now that we know about Dirichlet processes, we will put a DP prior on the partition of our data into clusters. Let us put this model in a probabilistic framework: let each cluster represent a univariate Gaussian distribution; this model will then represent an infinite Gaussian mixture model. In this mapping to our Chinese Restaurant Process metaphor, a table at a restaurant is a cluster, and a data point is a customer.

\paragraph{A}
% Write out the generative model. What is a simple choice of base distribution?
The generative model for a continuous $\eta$, a base distribution $G_0$, concentration parameter $\alpha$, and the $\{B_1,...B_K\}$ partitions ($K=\inf$), is:
\begin{eqnarray*}
(G(\eta \in B_1), ..., G(\eta \in B)K) &\sim& \text{Dirich}(\alpha G_0 (B_1), ... \alpha G_0 (B_K)) \\
p(\eta_i \in B_j) &=& \int p(\eta_i \in B_j | G) p(G|G_0) dG \\
&=& {\alpha G_0 (B_j) \over \sum_{K} \alpha G_0 (B_k)} \\
&\propto& \alpha G_0 (B_j)  
\end{eqnarray*}

The posterior is
\begin{eqnarray*}
G|\eta_{1:n}, \alpha, G_0 &\sim& DP(\alpha, G_0 + \sum_{i=1}^n \delta_{\eta_i} (\eta))
\end{eqnarray*}

A simple choice for the base distribution is $G_0$ is the Gamma distribution, due to the conjugacy of the Gamma distribution with the Gaussian distribution. 

\paragraph{B}
% Write out the equation that you would use in your Gibbs sampler for cluster as- signment. Exploit exchangeability (i.e. treat each data point as if it were the last to arrive at the restaurant, then remove it from it’s current cluster assignment, and reassign it to a cluster using this equation).

For the cluster assignment step in the Gibbs sampler, we can exploit exchangeability. 


\begin{lstlisting}
cluster = function(x, centroids, alpha){
	# do the restaurant process 
	table_counts = restaurant(x, alpha)
	table_props = table_counts/sum(table_counts)
	num_tables = length(table_counts)

	# then exploit permutation
	permuted_x = sample(x)
	n = length(x)
	table_assignments = rep(NA, n)

	# pretend each x is last to arrive 
	for(i in 1:length(permuted_x)){
		table_i = sample(c(1:num_tables), 1, prob=table_props)
		table_assignments[i] = table_i 
	}
}

restaurant = function(x, alpha){
	table_counts = c(1) # number of 'customers' at each 'table'
											# first customer at first table 
	for(m in 2:n){
		tmp = c(table_counts, alpha)
		table_props = tmp/sum(tmp)

		# assign each 'customer' to a 'table' according to crp
		table_m = sample(c(1:length(tmp)), 1, prob=table_props)
		if(table_m==length(tmp)){ table_counts[table_m] = 1}
		else{ table_counts[table_m] = table_counts[table_m] + 1}
	}
	return(table_counts) # sufficient statistic
}
\end{lstlisting}

\paragraph{C}
% How does this algorithm handle empty clusters?

\paragraph{D}
% Explain how this model addresses the problems we encountered in Homework 4, including how this approach differs from specifying a penalty to cluster size of the form: ‘remove a cluster if it has fewer than γ points assigned to it’.

\end{answer}


\begin{answer}{2}
% You should now have dived into the analysis of your data and chosen a modeling framework and an validation approach. An important step in the research process is to look at what other ideas researchers have used to approach a related problem. In addition to making progress on your project this week, you will perform a literature search to see what other researchers have worked with similar data or related problems. What relevant approaches, feature sets, or kernels have they developed that might be useful in your own analysis? What modeling approaches and simplifying assumptions worked in this related work, and what didn’t work? What can you take from the scientific literature to your project? Write this up as a few paragraphs that will (as with last week’s homework) be included in your final project writeup; include citations.

\end{answer}



\end{document}